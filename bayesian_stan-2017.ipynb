{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loan pricing tool\n",
    "use loan, and term data to predict individual discount rates (core structural decision making parameters)\n",
    "simulate new products (with many parameters changed)\n",
    "### Price optimization dashboard\n",
    "product portfolio with competing products (e.g P&G) -- risk of cannibalisation\n",
    "RCT, AB testing is common --> price elasticity identifical (partial revenue curve)\n",
    "simulate product substitution\n",
    "*publishing house*: max revenues\n",
    "*author*: different goals, contract to sign off on prices; publishing house can run constrained optimisation on other prices\n",
    "structural stuff\n",
    "### British Election, YouGov \n",
    "pure prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Why learn Bayesian?\n",
    "## Benefit 1:\n",
    "incorporate knowledge form outside the data\n",
    "#### example: Eight Schools\n",
    "limited size studies\n",
    "consider the 9th school (I)\n",
    "## Benefit 2: Combining sources of info\n",
    "## Benefit 3: Dealing with uncertainty consistently in model predictions\n",
    "## Benefit 4: Regularising richly parameterised models\n",
    "UPS; 25000 x 25000 postcodes; with missing pairwise cells\n",
    "random coefficients > dummies\n",
    "## Benefit 5: Doing away with tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Models and inference\n",
    "generative modelling\n",
    "X - treated as known out of sample\n",
    "knowing $\\beta$, X and $\\sigma$; can generate data\n",
    "\n",
    "have models & should use bayesian estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Why use Stan?\n",
    "\n",
    "PyMC3, Stan, Edward\n",
    "Cross-platform -- use R, Python for data prep & data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.1 tau ~ cauchy(0, 1); change to tau ~ normal(0,1)\n",
    "# rudy kalman filter\n",
    "# no need for prior for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Choice\n",
    "consumer choice models -- preferences (invisibilia)  \n",
    "$V_{ijt} = U_{ijt} + \\epsilon_{ijt}$  \n",
    "$U_{ijt}$ : price, product characteristics, preferences, latent \"demand shock\"  \n",
    "$U_{ijt} = \\alpha_i p_{jt} + X_{jt} \\beta_i + \\xi_{jt}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use $\\alpha, \\beta$ to generate purchase choices for individuals \n",
    "summarise probabilities for individuals\n",
    "compare simulated probabilties with actual purchases (sum log(probability(actual choice))) = likelihood value)\n",
    "\n",
    "without individual level; you get mkt level statistics\n",
    "average across people up to market shares --> multinomial distribution ($s_{jt}$)\n",
    "let $sales_t = (sales_{jt}) \\sim \\text{multinomial} (s_t(X,p,\\xi,\\alpha_i,\\beta_i), size)$\n",
    "true market share of 10%, but actual sales will be distributed around market share\n",
    "\n",
    "can estimate joint dist of $\\alpha, \\beta$, even without individual level data, with some modelling assumptions\n",
    "1. multivariate normal\n",
    "2. $\\Omega = L_\\Omega' L_\\Omega$\n",
    "3. $\\alpha \\sim N(\\mu,\\sigma) \\Rightarrow \\alpha = \\mu + \\sigma z, z \\sim (0,1)$\n",
    "\n",
    "transforms $s(p,X,\\xi, \\alpha_i, \\beta_i) \\rightarrow s(p,X,\\xi,\\alpha,\\beta,L_\\Omega,\\tau)$\n",
    "\n",
    "*accounting for endogeneity to remove $\\alpha$ bias*\n",
    "$P_{jt} \\sim \\text{a model} (X, Z, \\xi)$\n",
    "e.g. $P_{jt} \\sim N_t(X_{jt} \\Gamma + Z_{jt} \\Delta + \\lambda \\xi_{jt}, \\sigma)$\n",
    "\n",
    "then use covariance structure of preferences\n",
    "then can ask \"what product to introduce?\" (product as represented in characteristic space)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no need to run individual simulations; if assume errors are Gumbel/Generalized Extreme Value distribution Type-I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference in:\n",
    "1. optimisation; confidence interval around peak of the posterior (similar to loglikelihood)\n",
    "2. variational bayes; approximate distribution function (e.g. normal)\n",
    "3. HMC; histogram from draws\n",
    "\n",
    "neutral net = generative model\n",
    "with priors you could generate neutral nets but currently not computational feasible right now\n",
    "\n",
    "sampling uncertainty, and inferential uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
